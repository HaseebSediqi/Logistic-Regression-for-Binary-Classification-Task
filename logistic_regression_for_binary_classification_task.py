# -*- coding: utf-8 -*-
"""Logistic Regression for Binary Classification Task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S0uRmICDgSgZZKabLfu4wq1PtSyVIQu6
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.metrics import roc_curve, roc_auc_score
data = pd.read_csv("advertising.csv")

"""Data Analysis and Exploration"""

data.head()

data.isnull().sum() # checking for null/na values

data.shape # number of rows and columns

data.info() # detailed information about data

data.describe() # Arithemtics of data

# Plot Histogram of Age distribution
plt.figure(figsize=(8, 6))
data['Age'].hist(bins=20, color='skyblue', edgecolor='black')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.title('Distribution of Age')
plt.grid(True)
plt.show()

# Jointplot Income vs Age
plt.figure(figsize=(8, 6))
sns.jointplot(x=data["Area Income"], y=data.Age)

# Joint plot: Time Spent on Site vs Age
plt.figure(figsize=(8,6))
joint_plot =sns.jointplot(x=data['Daily Time Spent on Site'],y=data['Age'],kind='hex')
joint_plot.set_axis_labels(xlabel='Daily Time Spent on Site (minutes)', ylabel='Age (years)')

# Adding 'Gender' column based on 'Male' column
data['Gender'] = data['Male'].map({0:'Female',1:'Male'})

# Checking which gender uses the internet more
gender_usage = data.groupby('Gender')['Daily Internet Usage'].mean().reset_index()
plt.figure(figsize = (6,6))
plt.pie(gender_usage['Daily Internet Usage'], labels=gender_usage['Gender'], autopct='%1.1f%%', colors=['lightcoral', 'skyblue'])
plt.title('Proportion of Average Daily Internet Usage by Gender')
plt.show()

# Count of clicks on Ad
plt.figure(figsize=(6,4))
sns.countplot(x='Clicked on Ad',data=data,palette='deep')

# Pairplot colored by 'Clicked on Ad'
sns.pairplot(data,hue='Clicked on Ad')

# Scatter plot with FacetGrid

g = sns.FacetGrid(data, hue='Clicked on Ad', height=5)
g.map(sns.scatterplot, 'Daily Time Spent on Site', 'Age').add_legend()
plt.show()

# Heat Map for correlation
numeric_data = data.select_dtypes(include='number')
plt.figure(figsize=(10, 7))
sns.heatmap(numeric_data.corr(), annot=True)

"""We are going to use Logistic Regression to classify whehter a particular user will click on ad or no.
Logistic Regression is a statistical model used for binary classification tasks. The sigmoid function, also known as the logistic function, is crucial here. It maps any input value to a range between 0 and 1, making it suitable for modeling probabilities.
"""

# Sigmoid function plot (used in Logistic Regression)
def sigmoid(x):
   return 1 / (1 + np.exp(-x))

x = np.linspace(-10,10,100)
y = sigmoid(x)

plt.figure(figsize=(8,6))
plt.plot(x,y, color ="darkblue")
plt.title('Sigmoid Function')
plt.xlabel("x")
plt.ylabel("f(x)")
plt.grid(True)
plt.show()

"""Feature Selection , Model Training, Testing and Evaluation"""

# Feature Selection and Data Splitting
features = data.drop(['Clicked on Ad','City','Timestamp','Country','Ad Topic Line','Gender'],axis = 1)
response = data['Clicked on Ad']
x_train,x_test,y_train,y_test = train_test_split (features,response,test_size = 0.2,random_state = 42)

# Logistic Regression Model
model = LogisticRegression()
model.fit(x_train,y_train)

# Predictions
y_pred = model.predict(x_test)

# evaluating performance of model
accuracy_model = accuracy_score(y_pred,y_test)
print(accuracy_model)

# Confusion Matrix
cm = confusion_matrix(y_pred,y_test)
print("Confusion Matrix:",cm)

# Classification Report
report_classification = classification_report(y_pred,y_test)
print("Classification Report: ",report_classification)

# Feature Coefficients
coef = pd.DataFrame(model.coef_, columns=features.columns)
print("Coefficients:")
print(coef)

"""Hyperparameter Tuning"""

# Define the hyperparameters grid
param_grid = {
    'C':[0.01,0.1,1,10,100],
    'penalty':['l1','l2'],
    'solver':['liblinear']
}

log_regression = LogisticRegression(max_iter=300)
grid_search = GridSearchCV(log_regression, param_grid, cv=5, scoring='accuracy')
grid_search.fit(x_train,y_train)
print("Best Parameters: ",grid_search.best_params_)
print("Best Cross_Validation Accuracy: ",grid_search.best_score_)

best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(x_test)
accuracy_tune = accuracy_score(y_test,y_pred_best)
print("Accuracy Score:", accuracy_tune)
confusion_matrix_best = confusion_matrix(y_test,y_pred_best)
print("Confustion Matrix: ",confusion_matrix_best)
print("Classification Report: ",classification_report(y_test,y_pred_best))

# Get the predicted probabilities
y_probs = best_model.predict_proba(x_test)[:,1]
# Compute ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_probs) # Probabilities of the positive class (clicked on ad)
roc_auc = roc_auc_score(y_test, y_probs)

# Plot ROC curve
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()